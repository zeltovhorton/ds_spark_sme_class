{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - exploring the class datasets with IPython Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime Dataset\n",
    "The city of San Francisco released an interesting dataset that lists all crimes committed in the city since 2003, as reported by SFPD. The dataset is freely available on the web at: https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry\n",
    "\n",
    "We have pre-loaded this \"crime event\" dataset into HDFS. It's a single file called \"sf_crimes.csv\" under folder \"crime_report\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, as usual, setup a Spark Context, but since we are working with larger datasets this time -- configure Spark to create 15 executors with 2GB for executor memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up Spark Context\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Spark program that reads this CSV file into an RDD. Each item in this RDD is a list of all the field values for this line. \n",
    "\n",
    "A few notes:\n",
    "1. The file's first row includes the column names, which are not values, so you may want to ignore this first line. Think about a simple way to filter that line out.\n",
    "2. Some field values include commas inside quotes, so you have to deal with this appropriately. Look at Python's CSV package (https://docs.python.org/2/library/csv.html) for a simple solution, but bear in mind that csv.reader returns an object that might not be serializable by PySpark, so you'll have to use Python's next() to iterate over it inside your map() function.\n",
    "\n",
    "Print the total number of crime events in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "lines = sc.textFile(\"crime_report/sf_crimes.csv\") \\\n",
    "          <YOUR CODE HERE TO FILTER HEADER ROW>\n",
    "crimes = <YOUR CODE HERE>\n",
    "crimes.cache() \n",
    "print \"number of crimes reported = \", crimes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print 5 example rows from the RDD, using Pandas.\n",
    "1. Use pd.DataFrame() to convert the first 5 elements of the RDD into a pandas DataFrame\n",
    "2. Column names are: id, category, description, dayofweek, date, time, district, resolution, address, x, y, location, pdid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(<YOUR CODE HERE>)                  \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"category\" field has multiple different values it may contain. \n",
    "Write a Spark program that counts the number of crimes of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Panda's plot function to display the count per category as a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a program that counts the number of crimes per district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use Panda's plot function to display the count per category as a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a weather dataset that includes various measurements from weather stations around the country. This dataset is loaded into HDFS under the \"weather\" folder.\n",
    "\n",
    "Write a Spark program that loads the weather data for 2013 (under the path \"weather/2013.csv\") into an RDD.\n",
    "Note that the file has comma-separated records, each with 8 fields, but we only care about the first 4 (in order):\n",
    "- station name\n",
    "- date\n",
    "- metric type\n",
    "- value\n",
    "\n",
    "Use pandas pd.DataFrame(...) function to translate the result into a Pandas dataframe that includes the first 5 elements in this RDD and print the resulting Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lines = sc.textFile(\"weather/2013.csv\")\n",
    "weather = <YOUR CODE HERE>\n",
    "weather.cache()\n",
    "pd.DataFrame(weather.take(5), columns = ['station', 'date', 'metric', 'value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of unique weather stations in this RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of readings for each metric. Which are the top-10 most popular metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and plot the maximum daily temparature (TMAX) in San Francisco (weather station USW00023272) during the month of December 2013:\n",
    "* First filter the dataset to the SF weather station and to the TMAX metric\n",
    "* use map() to generate an RDD with a tuple of: (1) day (2) TMAX value for that day\n",
    "* Turn the RDD into a Pandas dataframe and print/plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
