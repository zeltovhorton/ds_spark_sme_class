{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Joins with Spark and DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at Joins with Spark DataFrames. \n",
    "First, as always we create a SparkContext and from it a HiveContext, using the \"demo\" database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up Spark Context\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.instances', 15)\n",
    "conf.set('spark.sql.autoBroadcastJoinThreshold', 200*1024*1024)  # 200MB for broadcast join\n",
    "sc = SparkContext('yarn-client', 'Spark-lab8', conf=conf)\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "hc = HiveContext(sc)\n",
    "hc.sql(\"use demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the weather dataset to include only data from 2013 and the San Francisco weather station (USW00023272).\n",
    "Then, create three dataframes:\n",
    "1. Daily percipitation (PRCP)\n",
    "2. Daily lowest temparature (TMIN)\n",
    "3. Daily highest temparature (TMAX)\n",
    "\n",
    "You might want to consider renaming the \"value\" column in each subset to reflect the type of value it represents. For example, in the prcp table you may rename value to prcp_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather = hc.sql(<YOUR SQL CODE HERE>).cache()\n",
    "prcp = weather.<YOUR CODE HERE>\n",
    "tmin = weather.<YOUR CODE HERE>\n",
    "tmax = weather.<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join all the metric-specific dataframes into a new dataframe with the following fields: year, month, day, prcp, tmin, tmax. \n",
    "\n",
    "Print the first 5 rows of this merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wdata = <YOUR CODE HERE>\n",
    "wdata.cache()\n",
    "wdata.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join the resulting dataframe (wdata) with the crimes dataframe, using the join key: month and day. Then print the explain() and notice how Spark uses a broadcast join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "crimes = hc.<YOUR CODE HERE>\n",
    "jdata = <YOUR CODE HERE>\n",
    "print jdata.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now measure (using \"%%timeit\") the time it takes to execute the join by printing the first 5 rows in the resuling dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "jdata.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Setup a new SparkContext with Broadcast join disabled:\n",
    "\n",
    "     conf.set('spark.sql.autoBroadcastJoinThreshold', -1)  # Disable broadcast join\n",
    "\n",
    "Remember to use sc.stop() to close the original spark context and discard its hold on cluster resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.<YOUR CODE HERE>\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '4g')\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.instances', 8)\n",
    "conf.set('spark.sql.autoBroadcastJoinThreshold', -1)  # Disable broadcast join\n",
    "sc = SparkContext('yarn-client', 'lab8', conf=conf)\n",
    "hc = HiveContext(sc)\n",
    "hc.sql(\"use demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the complete code sequence again using this new Spark Context (with broadcast join disabled), and use explain() to verify Spark is now using a ShuffledHashJoin instead of Broadcast join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the time (using \"%%timeit\") it takes to execute without a broadcast join and compare to the previous result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "jdata.limit(5).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
