{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: geo-spatial aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will further explore and analyze the crimes dataset for data quality issues, and then use geo-spatial analysis to determine the neighborhood associated with each crime event, based on its longitude/latitude coordinates. We then use Folium to plot the data on an interactive map.\n",
    "\n",
    "First, setup the Spark Context, and create a HiveContext using the \"demo\" table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up Spark Context\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.instances', 15)\n",
    "sc = SparkContext('yarn-client', 'Spark-lab9', conf=conf)\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "hc = HiveContext(sc)\n",
    "hc.sql(\"use demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always good to inspect data for quality. We would like to do this for the longitude/latitude data in our dataset.\n",
    "\n",
    "1. Load the crimes dataset as a Spark DataFrame\n",
    "2. Use describe() to inspect the properties of the columns 'longitude' and 'latitude'\n",
    "\n",
    "describe() computes summary statistics for each numeric feature in the Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crimes = hc.<YOUR CODE HERE>\n",
    "crimes.<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which values stand out as abnormal, considering the general longitude/latitude values in San Francisco?\n",
    "\n",
    "Assuming all anomalies are of similar nature, let's explore how many outliers like this exist. \n",
    "* Create a data frame with all these outliers\n",
    "* Count how many exist\n",
    "* Print 3 outlier rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outliers = crimes.<YOUR CODE HERE>\n",
    "print \"number of outliers = %d\" % outliers.count()\n",
    "\n",
    "outliers.select(\"category\", \"description\", \"date_str\", \"longitude\", \"latitude\").limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move to some geo-spatial aggregation. The goal is to use ESRI's HIVE UDFs to determine the neighborhood for each crime event, by its longitude/latitude coordinates.\n",
    "You can find more information about ESRI Hive UDFs here: https://github.com/Esri/spatial-framework-for-hadoop\n",
    "\n",
    "Notes:\n",
    "* The neighborhood polygon definitions have already been uploaded to HIVE as the table *sf_neighborhoods*, so we can use the ESRI Hive UDF functions to determine the neighborhood name for each crime.\n",
    "* Remember to filter the data so as to remove any events with anomalous longitude/latitude values.\n",
    "* Notice the \"repartition(50)\" - this is to increase parallelism and make this query faster Spark SQL.\n",
    "* We add the various jars to make ESRI UDFs work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hc.sql(\"add jar /home/jupyter/notebooks/jars/guava-11.0.2.jar\")\n",
    "hc.sql(\"add jar /home/jupyter/notebooks/jars/esri-geometry-api.jar\")\n",
    "hc.sql(\"add jar /home/jupyter/notebooks/jars/spatial-sdk-hive.jar\")\n",
    "hc.sql(\"add jar /home/jupyter/notebooks/jars/spatial-sdk-json.jar\")\n",
    "\n",
    "hc.sql(\"create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains'\")\n",
    "hc.sql(\"create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point'\")\n",
    "\n",
    "cf = hc.sql(\"\"\"\n",
    "SELECT  date_str, time, longitude, latitude, resolution, category, district, dayofweek, description\n",
    "FROM crimes\n",
    "WHERE longitude < -121.0 and latitude < 38.0\n",
    "\"\"\").repartition(50)\n",
    "cf.registerTempTable(\"cf\")\n",
    "\n",
    "crimes_wn = hc.sql(\"\"\"\n",
    "SELECT date_str, time, dayofweek, category, district, resolution, description, longitude, latitude,\n",
    "       neighborho as neighborhood \n",
    "FROM sf_neighborhoods JOIN cf\n",
    "WHERE ST_Contains(sf_neighborhoods.shape, ST_Point(cf.longitude, cf.latitude))\n",
    "\"\"\").cache()\n",
    "\n",
    "crimes_per_neighborhood = crimes_wn.groupBy('neighborhood').count().toPandas()\n",
    "print crimes_per_neighborhood.sort(columns='count', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the updated crimes dataset with neighborhood names into an ORC table in HIVE called \"crimes_wn\", using Spark's DataFrameWriter API and the saveAsTable() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crimes_wn.<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the inline_map() helper function to draw maps with Folium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "map_width=1000\n",
    "map_height=600\n",
    "\n",
    "def inline_map(m, width=map_width, height=map_height):\n",
    "    m.create_map()\n",
    "    srcdoc = m.HTML.replace('\"', '&quot;')\n",
    "    embed = HTML('<iframe srcdoc=\"{}\" '\n",
    "                 'style=\"width: {}px; height: {}px; '\n",
    "                 'border: none\"></iframe>'.format(srcdoc, width, height))\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Folium package to draw a map centered at the heart of San Francisco (Latitude 37.77, Longitude -122.4), and specify a starting zoom level of 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "sf_lat = 37.77\n",
    "sf_long = -122.4\n",
    "\n",
    "map_sf = folium.<YOUR CODE HERE>\n",
    "inline_map(map_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have pre-loaded into the \"data\" folder a GeoJSON file that includes the neigbordhood boundaries of all San Francisco neighborhoods. Use Folium's geo_json function to draw the boundaries on the map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_sf = folium.<YOUR CODE HERE>\n",
    "map_sf.geo_json(<YOUR CODE HERE>)\n",
    "inline_map(map_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using crimes_per_neighborhood we computed earlier, plot a map color-coded with the number of crimes in each neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_sf = folium.<YOUR CODE HERE>\n",
    "map_sf.<YOUR CODE HERE>\n",
    "inline_map(map_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ESRI's HIVE UDFs to compute the centroid of each neighborhood, and then plot a Folium map with a simple_marker for each neighborhood, displaying the neighborhood name and number of crimes in that neighborhood: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hc.sql(\"create temporary function ST_Centroid as 'com.esri.hadoop.hive.ST_Centroid'\")\n",
    "hc.sql(\"create temporary function ST_X as 'com.esri.hadoop.hive.ST_X'\")\n",
    "hc.sql(\"create temporary function ST_Y as 'com.esri.hadoop.hive.ST_Y'\")\n",
    "\n",
    "rdd_centroid = hc.sql(\"\"\"\n",
    "SELECT neighborho as neighborhood, \n",
    "       ST_X(ST_Centroid(sf_neighborhoods.shape)) as cent_longitude,\n",
    "       ST_Y(ST_Centroid(sf_neighborhoods.shape)) as cent_latitude\n",
    "FROM sf_neighborhoods\n",
    "\"\"\")\n",
    "\n",
    "map_sf = folium.Map(location=[sf_lat, sf_long], zoom_start=12, width=map_width, height=map_height)\n",
    "s = pd.Series(index=crimes_per_neighborhood['neighborhood'].values, \\\n",
    "              data=crimes_per_neighborhood['count'].values.astype(float))\n",
    "\n",
    "for n in rdd_centroid.collect():\n",
    "    map_sf.simple_marker(<YOUR CODE HERE>)\n",
    "    \n",
    "inline_map(map_sf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
