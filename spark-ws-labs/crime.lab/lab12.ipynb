{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12: Predictive model with Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we create a SparkContext/HiveContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up Spark Context\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.instances', 15)\n",
    "conf.set('spark.sql.autoBroadcastJoinThreshold', 100*1024*1024)  # 100MB for broadcast join\n",
    "sc = SparkContext('yarn-client', 'Spark-lab12', conf=conf)\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "hc = HiveContext(sc)\n",
    "hc.sql(\"use demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_metrics(lap):\n",
    "    tp = float(len(lap[(lap['label']==1) & (lap['prediction']==1)]))\n",
    "    tn = float(len(lap[(lap['label']==0) & (lap['prediction']==0)]))\n",
    "    fp = float(len(lap[(lap['label']==0) & (lap['prediction']==1)]))\n",
    "    fn = float(len(lap[(lap['label']==1) & (lap['prediction']==0)]))\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
    "    return {'precision': precision, 'recall': recall, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with lab11:\n",
    "1. Load the feature matrix created in lab 10 into a Spark dataframe called 'fm'\n",
    "2. Split into two dataframes - train (2011-2013) and test (only 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a pipline very similar to lab 11, only this time we use Random Forest instead of Logistic regression.\n",
    "For parameters to the random forest, you can use: numTrees=100, maxDepth=4, maxBins=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now modify this pipeline to also add up to 50 features corresponding to words in the \"description\" field. Use Tokenizer and HashingTF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the random forest and evaluate the results using the eval_metrics() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the overall accuracy metrics, we now want to plot the accuacy of prediction per each neighborhood. In order to do this, we first use ESRI's HIVE UDFs: ST_X, ST_Y nad ST_Centroid to compute the longitude/latitude centroid of each neighborhood in San Francisco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hc.sql(\"add jar /home/jupyter/notebooks/jars/guava-11.0.2.jar\")\n",
    "hc.sql(\"add jar /home/jupyter/notebooks/jars/esri-geometry-api.jar\")\n",
    "hc.sql(\"add jar /home/jupyter/notebooks/jars/spatial-sdk-hive.jar\")\n",
    "hc.sql(\"add jar /home/jupyter/notebooks/jars/spatial-sdk-json.jar\")\n",
    "\n",
    "hc.sql(\"create temporary function ST_Centroid as 'com.esri.hadoop.hive.ST_Centroid'\")\n",
    "hc.sql(\"create temporary function ST_X as 'com.esri.hadoop.hive.ST_X'\")\n",
    "hc.sql(\"create temporary function ST_Y as 'com.esri.hadoop.hive.ST_Y'\")\n",
    "\n",
    "df_centroid = hc.sql(\"\"\"\n",
    "SELECT neighborho as neighborhood, \n",
    "       ST_X(ST_Centroid(sf_neighborhoods.shape)) as cent_longitude,\n",
    "       ST_Y(ST_Centroid(sf_neighborhoods.shape)) as cent_latitude\n",
    "FROM sf_neighborhoods\n",
    "\"\"\")\n",
    "df_centroid.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the centroid for each neighborhood, we compute the accuracy of prediction specific to crimes within that neighborhood. \n",
    "\n",
    "Complete the code below to plot a map of san francisco, with markers in the centroid of each neighborhood showing that accuracy number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "from IPython.display import HTML\n",
    "map_width=1000\n",
    "map_height=600\n",
    "sf_lat = 37.77\n",
    "sf_long = -122.4\n",
    "\n",
    "def inline_map(m, width=map_width, height=map_height):\n",
    "    m.create_map()\n",
    "    srcdoc = m.HTML.replace('\"', '&quot;')\n",
    "    embed = HTML('<iframe srcdoc=\"{}\" '\n",
    "                 'style=\"width: {}px; height: {}px; '\n",
    "                 'border: none\"></iframe>'.format(srcdoc, width, height))\n",
    "    return embed\n",
    "\n",
    "\n",
    "n_list = results.select(\"neighborhood\").distinct().toPandas()['neighborhood'].tolist()\n",
    "\n",
    "df = results.select(\"neighborhood\", \"label\", \"prediction\").toPandas()\n",
    "map_sf = folium.Map(location=[sf_lat, sf_long], zoom_start=12, width=map_width, height=map_height)\n",
    "for n in df_centroid.collect():\n",
    "    if n.neighborhood in n_list:\n",
    "        m = eval_metrics(df[df['neighborhood']==n.neighborhood])\n",
    "        map_sf.simple_marker(<YOUR CODE HERE>)\n",
    "\n",
    "inline_map(map_sf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
