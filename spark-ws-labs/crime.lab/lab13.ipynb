{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13: Selecting a model with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we create a SparkContext/HiveContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up Spark Context\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.instances', 15)\n",
    "conf.set('spark.sql.autoBroadcastJoinThreshold', 100*1024*1024)  # 100MB for broadcast join\n",
    "sc = SparkContext('yarn-client', 'Spark-lab13', conf=conf)\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "hc = HiveContext(sc)\n",
    "hc.sql(\"use demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_metrics(lap):\n",
    "    tp = float(len(lap[(lap['label']==1) & (lap['prediction']==1)]))\n",
    "    tn = float(len(lap[(lap['label']==0) & (lap['prediction']==0)]))\n",
    "    fp = float(len(lap[(lap['label']==0) & (lap['prediction']==1)]))\n",
    "    fn = float(len(lap[(lap['label']==1) & (lap['prediction']==0)]))\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
    "    return {'precision': precision, 'recall': recall, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, prepare the training and testing datasets:\n",
    "1. Load the feature matrix created in lab 10 into a Spark dataframe called 'fm'\n",
    "2. Split into two dataframes - train (2011-2013) and test (only 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in lab 12, build instances of the StringIndexer() and OneHotEncoder() for each of the variables, then combine them with a VectorAssembler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Build pre-process pipeline\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the complete feature generation and cross validation pipeline:\n",
    "* Build a CrossValidator instance using LogisticRegression and a paramter grid. In your paramter grid, test the values [0.1, 0.5, 1.0, 5.0] for regularization paramter, and the values [0.0, 0.5, 1.0] for the elasticNetParam.\n",
    "* Build a pipeline with all the stages plus the cross validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Build a parameter grid\n",
    "cvlr = LogisticRegression(maxIter=100)\n",
    "grid = (ParamGridBuilder() \n",
    "      .addGrid(<YOUR CODE HERE>) \n",
    "      .addGrid(<YOUR CODE HERE>)\n",
    "      .build())\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "cv = CrossValidator(<YOUR CODE HERE>)\n",
    "\n",
    "pipe = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the end-to-end pipeline:\n",
    "* Store the output of fit() in a variable called \"model\"\n",
    "* Apply the fitted model to the test data\n",
    "* Compute and print the resulting metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = pipe.<YOUR CODE HERE>\n",
    "results = model.<YOUR CODE HERE>\n",
    "\n",
    "print \"best AUC-ROC = \" + str(evaluator.evaluate(results))\n",
    "print eval_metrics(<YOUR CODE HERE>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"model\" variable is of type PipelineModel and includes the fitted stages of each stage in the pipeline. We can look at the final stage (our cross-validation model) and print the intercept and weights for the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestModel = model.<YOUR CODE HERE>\n",
    "print bestModel.intercept\n",
    "print bestModel.weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
