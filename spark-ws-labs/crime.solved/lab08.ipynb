{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Joins with Spark and DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at Joins with Spark DataFrames. \n",
    "First, as always we create a SparkContext and from it a HiveContext, using the \"demo\" database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[result: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up Spark Context\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.instances', 15)\n",
    "conf.set('spark.sql.autoBroadcastJoinThreshold', 200*1024*1024)  # 200MB for broadcast join\n",
    "sc = SparkContext('yarn-client', 'Spark-lab8', conf=conf)\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "hc = HiveContext(sc)\n",
    "hc.sql(\"use demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the weather dataset to include only data from 2013 and the San Francisco weather station (USW00023272).\n",
    "Then, create three dataframes:\n",
    "1. Daily percipitation (PRCP)\n",
    "2. Daily lowest temparature (TMIN)\n",
    "3. Daily highest temparature (TMAX)\n",
    "\n",
    "You might want to consider renaming the \"value\" column in each subset to reflect the type of value it represents. For example, in the prcp table you may rename value to prcp_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather = hc.sql(\"select * from weather WHERE year==2013 and station == 'USW00023272'\").cache()\n",
    "prcp = weather.filter(weather.metric=='PRCP').withColumnRenamed('value', 'prcp_val')\n",
    "tmin = weather.filter(weather.metric=='TMIN').withColumnRenamed('value', 'tmin_val')\n",
    "tmax = weather.filter(weather.metric=='TMAX').withColumnRenamed('value', 'tmax_val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join all the metric-specific dataframes into a new dataframe with the following fields: year, month, day, prcp, tmin, tmax. \n",
    "\n",
    "Print the first 5 rows of this merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>prcp</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  prcp  tmin  tmax\n",
       "0  2013      2   12     0    72   156\n",
       "1  2013      8   25     0   156   206\n",
       "2  2013     11    3     0   100   161\n",
       "3  2013      6    8     0   111   167\n",
       "4  2013      2   26     0    78   167"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdata = prcp.join(tmin, 'date_str').join(tmax, 'date_str') \\\n",
    "            .select(prcp.year.alias('year'), prcp.month.alias('month'), prcp.day.alias('day'), \\\n",
    "                    prcp.prcp_val.alias('prcp'), tmin.tmin_val.alias('tmin'), tmax.tmax_val.alias('tmax'))\n",
    "wdata.cache()\n",
    "wdata.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join the resulting dataframe (wdata) with the crimes dataframe, using the join key: month and day. Then print the explain() and notice how Spark uses a broadcast join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BroadcastHashJoin [month#70,day#71], [month#240,day#241], BuildLeft\n",
      " ConvertToUnsafe\n",
      "  InMemoryColumnarTableScan [year#69,month#70,day#71,prcp#72,tmin#73,tmax#74], (InMemoryRelation [year#69,month#70,day#71,prcp#72,tmin#73,tmax#74], true, 10000, StorageLevel(true, true, false, true, 1), (TungstenProject [year#11 AS year#69,month#12 AS month#70,day#13 AS day#71,prcp_val#52 AS prcp#72,tmin_val#53 AS tmin#73,tmax_val#54 AS tmax#74]), None)\n",
      " ConvertToUnsafe\n",
      "  InMemoryColumnarTableScan [incidentid#243,category#244,description#245,dayofweek#246,date_str#247,time#248,district#249,resolution#250,address#251,longitude#252,latitude#253,location#254,pdid#255,month#240,day#241,year#242], (InMemoryRelation [incidentid#243,category#244,description#245,dayofweek#246,date_str#247,time#248,district#249,resolution#250,address#251,longitude#252,latitude#253,location#254,pdid#255,month#240,day#241,year#242], true, 10000, StorageLevel(true, true, false, true, 1), (Project [incidentid#243,category#244,description#245,dayofweek#246,date_str#247,time#248,district#249,resolution#250,address#251,longitude#252,latitude#253,location#254,pdid#255,cast(cast(substring(date_str#247,1,2) as decimal(20,0)) as int) AS month#240,cast(cast(substring(date_str#247,4,2) as decimal(20,0)) as int) AS day#241,cast(cast(substring(date_str#247,7,4) as decimal(20,0)) as int) AS year#242]), None)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "crimes = hc.sql(\"\"\"\n",
    "SELECT *, \n",
    "       cast(substr(date_str,1,2) as int) as month, \n",
    "       cast(substr(date_str,4,2) as int) as day, \n",
    "       cast(substr(date_str,7,4) as int) as year \n",
    "FROM crimes \n",
    "WHERE substr(date_str,7,4)=='2013'\n",
    "\"\"\").cache()\n",
    "\n",
    "jdata = wdata.join(crimes, (wdata.month==crimes.month) & (wdata.day==crimes.day))  # Parens are necessary in expr\n",
    "print jdata.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now measure the time it takes to execute the join by printing the first 5 rows in the resuling dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 1: 19.4 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "jdata.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Setup a new SparkContext with Broadcast join disabled:\n",
    "\n",
    "     conf.set('spark.sql.autoBroadcastJoinThreshold', -1)  # Disable broadcast join\n",
    "\n",
    "Remember to use sc.stop() to close the original spark context and discard its hold on cluster resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[result: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop()\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '4g')\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.instances', 8)\n",
    "conf.set('spark.sql.autoBroadcastJoinThreshold', -1)  # Disable broadcast join\n",
    "sc = SparkContext('yarn-client', 'lab8', conf=conf)\n",
    "hc = HiveContext(sc)\n",
    "hc.sql(\"use demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the complete code sequence again using this new Spark Context (with broadcast join disabled), and use explain() to verify Spark is now using a ShuffledHashJoin instead of Broadcast join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SortMergeJoin [month#626,day#627], [month#631,day#632]\n",
      " TungstenSort [month#626 ASC,day#627 ASC], false, 0\n",
      "  TungstenExchange hashpartitioning(month#626,day#627)\n",
      "   TungstenProject [year#567 AS year#625,month#568 AS month#626,day#569 AS day#627,prcp_val#608 AS prcp#628,tmin_val#609 AS tmin#629,tmax_val#610 AS tmax#630]\n",
      "    SortMergeJoin [date_str#570], [date_str#622]\n",
      "     TungstenProject [year#567,month#568,tmin_val#609,prcp_val#608,day#569,date_str#570]\n",
      "      SortMergeJoin [date_str#570], [date_str#615]\n",
      "       TungstenSort [date_str#570 ASC], false, 0\n",
      "        TungstenExchange hashpartitioning(date_str#570)\n",
      "         ConvertToUnsafe\n",
      "          Project [year#567,month#568,value#572 AS prcp_val#608,day#569,date_str#570]\n",
      "           Filter (metric#571 = PRCP)\n",
      "            InMemoryColumnarTableScan [year#567,value#572,month#568,day#569,metric#571,date_str#570], [(metric#571 = PRCP)], (InMemoryRelation [station#566,year#567,month#568,day#569,date_str#570,metric#571,value#572], true, 10000, StorageLevel(true, true, false, true, 1), (Filter ((year#567 = 2013) && (station#566 = USW00023272))), None)\n",
      "       TungstenSort [date_str#615 ASC], false, 0\n",
      "        TungstenExchange hashpartitioning(date_str#615)\n",
      "         ConvertToUnsafe\n",
      "          Project [date_str#615,value#617 AS tmin_val#609]\n",
      "           Filter (metric#616 = TMIN)\n",
      "            InMemoryColumnarTableScan [date_str#615,value#617,metric#616], [(metric#616 = TMIN)], (InMemoryRelation [station#611,year#612,month#613,day#614,date_str#615,metric#616,value#617], true, 10000, StorageLevel(true, true, false, true, 1), (Filter ((year#567 = 2013) && (station#566 = USW00023272))), None)\n",
      "     TungstenSort [date_str#622 ASC], false, 0\n",
      "      TungstenExchange hashpartitioning(date_str#622)\n",
      "       ConvertToUnsafe\n",
      "        Project [value#624 AS tmax_val#610,date_str#622]\n",
      "         Filter (metric#623 = TMAX)\n",
      "          InMemoryColumnarTableScan [value#624,date_str#622,metric#623], [(metric#623 = TMAX)], (InMemoryRelation [station#618,year#619,month#620,day#621,date_str#622,metric#623,value#624], true, 10000, StorageLevel(true, true, false, true, 1), (Filter ((year#567 = 2013) && (station#566 = USW00023272))), None)\n",
      " TungstenSort [month#631 ASC,day#632 ASC], false, 0\n",
      "  TungstenExchange hashpartitioning(month#631,day#632)\n",
      "   ConvertToUnsafe\n",
      "    Project [incidentid#634,category#635,description#636,dayofweek#637,date_str#638,time#639,district#640,resolution#641,address#642,longitude#643,latitude#644,location#645,pdid#646,cast(cast(substring(date_str#638,1,2) as decimal(20,0)) as int) AS month#631,cast(cast(substring(date_str#638,4,2) as decimal(20,0)) as int) AS day#632,cast(cast(substring(date_str#638,7,4) as decimal(20,0)) as int) AS year#633]\n",
      "     Filter (substring(date_str#638,7,4) = 2013)\n",
      "      HiveTableScan [date_str#638,location#645,longitude#643,address#642,resolution#641,time#639,latitude#644,district#640,pdid#646,dayofweek#637,incidentid#634,description#636,category#635], (MetastoreRelation demo, crimes, None)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "weather = hc.sql(\"select * from weather WHERE year==2013 and station == 'USW00023272'\").cache()\n",
    "prcp = weather.filter(weather.metric=='PRCP').withColumnRenamed('value', 'prcp_val')\n",
    "tmin = weather.filter(weather.metric=='TMIN').withColumnRenamed('value', 'tmin_val')\n",
    "tmax = weather.filter(weather.metric=='TMAX').withColumnRenamed('value', 'tmax_val')\n",
    "wdata = prcp.join(tmin, 'date_str').join(tmax, 'date_str') \\\n",
    "            .select(prcp.year.alias('year'), prcp.month.alias('month'), prcp.day.alias('day'), \\\n",
    "                    prcp.prcp_val.alias('prcp'), tmin.tmin_val.alias('tmin'), tmax.tmax_val.alias('tmax'))\n",
    "crimes = hc.sql(\"\"\"\n",
    "SELECT *, \n",
    "       cast(substr(date_str,1,2) as int) as month, \n",
    "       cast(substr(date_str,4,2) as int) as day, \n",
    "       cast(substr(date_str,7,4) as int) as year \n",
    "FROM crimes \n",
    "WHERE substr(date_str,7,4)=='2013'\n",
    "\"\"\")\n",
    "\n",
    "jdata = wdata.join(crimes, (wdata.month==crimes.month) & (wdata.day==crimes.day))\n",
    "print jdata.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the time (using \"%%timeit\") it takes to execute without a broadcast join and compare to the previous result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 1: 4min 23s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "jdata.limit(5).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
